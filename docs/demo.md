# ğŸ¯ æœ€ä¼˜å¼€å‘ç­–ç•¥ï¼šåŸºç¡€ â†’ è¿›é˜¶ï¼Œä¸€ä¸ªé¡¹ç›®ä¸¤æ¬¡èœ•å˜

## æ ¸å¿ƒæ€è·¯ï¼šä¸åšä¸¤ä¸ªé¡¹ç›®ï¼Œåšä¸€ä¸ªé¡¹ç›®çš„ä¸¤ä¸ªé˜¶æ®µ

> **åŸºç¡€é˜¶æ®µçš„ä»£ç  100% è¢«è¿›é˜¶é˜¶æ®µå¤ç”¨**ï¼Œä¸€è¡Œä¸æµªè´¹ã€‚

```
é˜¶æ®µ Aï¼ˆåŸºç¡€ï¼Œ2-3å°æ—¶ï¼‰                é˜¶æ®µ Bï¼ˆä¸­ç­‰ï¼Œåœ¨AåŸºç¡€ä¸ŠåŠ 4-5å°æ—¶ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬10é¢˜ï¼šAPIèšåˆå°è£…     â”‚  â”€â”€å‡çº§â”€â”€â†’  â”‚ ç¬¬21é¢˜ï¼šä¸ªæ€§åŒ–çŸ¥è¯†ç®¡ç†æ™ºèƒ½ä½“    â”‚
â”‚                      â”‚             â”‚                              â”‚
â”‚ âœ… å­¦ä¼š MCP æ¨¡æ¿æµç¨‹   â”‚             â”‚ âœ… æ–°å¢ ChromaDB å‘é‡æ•°æ®åº“    â”‚
â”‚ âœ… æå®š LLM å¯¹è¯      â”‚  ç›´æ¥å¤ç”¨â†’   â”‚ âœ… æ–°å¢ RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ       â”‚
â”‚ âœ… æå®šç¯å¢ƒé…ç½®/Git    â”‚  ç›´æ¥å¤ç”¨â†’   â”‚ âœ… æ–°å¢æ–‡æœ¬åˆ†å—å¤„ç†            â”‚
â”‚ âœ… ç†Ÿæ‚‰ tool/core å†™æ³• â”‚             â”‚ âœ… æœ€ç»ˆæäº¤è¿™ä¸ªç‰ˆæœ¬            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆè¿™æ˜¯æœ€ä¼˜ç­–ç•¥ï¼š**
- é˜¶æ®µ A çš„ `llm_service.py` åœ¨é˜¶æ®µ B ç›´æ¥ç”¨
- é˜¶æ®µ A çš„ config/Git/ç¯å¢ƒ åœ¨é˜¶æ®µ B ç›´æ¥ç”¨
- é˜¶æ®µ A åšå®Œä½ å°±å®Œå…¨ç†è§£äº† MCP æ¨¡æ¿çš„å¼€å‘æµç¨‹
- å¦‚æœæ—¶é—´ä¸å¤Ÿï¼Œäº¤é˜¶æ®µ A å°±æ˜¯åŸºç¡€é¢˜ï¼›æ—¶é—´å¤Ÿå°±äº¤é˜¶æ®µ B æ‹¿é«˜åˆ†

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é˜¶æ®µ Aï¼šåŸºç¡€é¢˜çƒ­èº«ï¼ˆç¬¬ 10 é¢˜ - API èšåˆå°è£…ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

> **ç›®æ ‡ï¼š** 2-3 å°æ—¶è·‘é€šæ•´ä¸ªæµç¨‹ï¼Œåšå‡ºä¸€ä¸ªèƒ½ç”¨çš„ MCP Server
> **æˆæœï¼š** ä¸€ä¸ªé›†æˆäº† LLM å¯¹è¯ + å¤©æ°”æŸ¥è¯¢ + ç¿»è¯‘çš„ MCP æœåŠ¡å™¨

---

## A-1: ç¯å¢ƒå‡†å¤‡ï¼ˆ15åˆ†é’Ÿï¼‰

### ç¡®è®¤å·¥å…·å·²å®‰è£…

```powershell
python --version    # éœ€è¦ 3.10+
uv --version        # éœ€è¦ uv
git --version       # éœ€è¦ Git
node --version      # éœ€è¦ Node.jsï¼ˆMCP Inspector ç”¨ï¼‰
```

å¦‚æœç¼ºå°‘å·¥å…·ï¼š
```powershell
# å®‰è£… uv
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Git å’Œ Node.js å»å®˜ç½‘ä¸‹è½½å®‰è£…
# https://git-scm.com
# https://nodejs.org
```

---

## A-2: ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆ10åˆ†é’Ÿï¼‰

### ä¿®æ”¹ `config.yaml`

```yaml
server:
  name: YA_MCPServer_KnowledgeAgent
  name_zh: ä¸ªæ€§åŒ–çŸ¥è¯†ç®¡ç†æ™ºèƒ½ä½“
  author: ä½ çš„åå­—
  description: A personalized knowledge management agent with RAG-based intelligent Q&A.
  description_zh: åŸºäº RAG çš„ä¸ªæ€§åŒ–çŸ¥è¯†ç®¡ç†æ™ºèƒ½ä½“ï¼Œæ”¯æŒçŸ¥è¯†å­˜å‚¨ã€è¯­ä¹‰æ£€ç´¢å’Œæ™ºèƒ½é—®ç­”ã€‚
  version: 0.1.0

transport:
  type: "sse"
  host: "127.0.0.1"
  port: 12345

logging:
  console:
    enabled: true
    level: "DEBUG"
  file:
    enabled: true
    level: "DEBUG"
    path: "logs/%Y-%m-%d_%H-%M-%S.log"
    rotation: "10 MB"
    retention: "7 days"
    compression: "zip"

# LLM é…ç½®
llm:
  default_provider: "deepseek"
  deepseek:
    base_url: "https://api.deepseek.com"
    model: "deepseek-chat"
    max_tokens: 2048
    temperature: 0.7
  openai:
    model: "gpt-3.5-turbo"
    max_tokens: 2048
    temperature: 0.7

# ç¿»è¯‘é…ç½®
translate:
  base_url: "https://api.mymemory.translated.net"
```

> ğŸ’¡ ä¸ºä»€ä¹ˆç›´æ¥ç”¨æœ€ç»ˆé¡¹ç›®åï¼Ÿå› ä¸ºé˜¶æ®µ A çš„ä»£ç ä¼šç›´æ¥å‡çº§ä¸ºé˜¶æ®µ Bï¼Œä¸ç”¨æ”¹ä¸¤æ¬¡åå­—ã€‚

### ä¿®æ”¹ `pyproject.toml`

```toml
[project]
name = "YA_MCPServer_KnowledgeAgent"
version = "0.1.0"
description = "A personalized knowledge management agent with RAG."
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "art>=6.5",
    "black>=25.9.0",
    "colorlog>=6.10.1",
    "httpx>=0.28.1",
    "mcp[cli]>=1.14.0",
    "pyyaml>=6.0.2",
    "ruff>=0.14.4",
    "openai>=1.0.0",
]
```

> âš ï¸ é˜¶æ®µ A å…ˆä¸è£… chromadbï¼Œä¿æŒä¾èµ–è½»é‡ï¼Œå¿«é€Ÿè·‘èµ·æ¥ã€‚

---

## A-3: Git åˆå§‹åŒ– + è™šæ‹Ÿç¯å¢ƒï¼ˆ10åˆ†é’Ÿï¼‰

```powershell
cd "d:\Syncthing Folder\Asus-Lenovo\School Projects\project_agent\YA_MCPServer_Template"

# Git åˆå§‹åŒ–
git init
git branch -M main
git add .
git commit -m "Initial Commit"

# åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b dev main

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv sync

# æ¿€æ´»
.venv\Scripts\activate

# éªŒè¯
python -c "import mcp; print('MCP OK')"
python -c "import openai; print('OpenAI OK')"
python -c "import httpx; print('HTTPX OK')"
```

---

## A-4: è¿è¡Œæ¨¡æ¿éªŒè¯ç¯å¢ƒï¼ˆ5åˆ†é’Ÿï¼‰

```powershell
uv run server.py
```

çœ‹åˆ°æœåŠ¡å™¨å¯åŠ¨ä¿¡æ¯å°±è¯´æ˜ç¯å¢ƒæ²¡é—®é¢˜ã€‚`Ctrl+C` åœæ­¢ã€‚

---

## A-5: å†™ç¬¬ä¸€ä¸ªæ ¸å¿ƒæ¨¡å— `core/llm_service.py`ï¼ˆ30åˆ†é’Ÿï¼‰

> è¿™æ˜¯é˜¶æ®µ A å’Œé˜¶æ®µ B éƒ½è¦ç”¨çš„æ ¸å¿ƒæ¨¡å—ï¼Œå†™ä¸€æ¬¡æ°¸ä¹…å¤ç”¨ã€‚

åˆ›å»ºæ–‡ä»¶ `core/llm_service.py`ï¼š

```python
"""
LLM æœåŠ¡æ¨¡å—

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- chat_with_llm: è°ƒç”¨ LLM API è¿›è¡Œå¯¹è¯ï¼ˆæ”¯æŒ DeepSeek / OpenAIï¼‰
"""

import os
from typing import Dict, Any, Optional, List
from openai import AsyncOpenAI
from modules.YA_Common.utils.config import get_config
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("llm_service")


async def chat_with_llm(
    message: str,
    system_prompt: Optional[str] = None,
    provider: Optional[str] = None,
) -> Dict[str, Any]:
    """
    è°ƒç”¨ LLM API è¿›è¡Œå¯¹è¯ã€‚

    Args:
        message (str): ç”¨æˆ·æ¶ˆæ¯ã€‚
        system_prompt (Optional[str]): ç³»ç»Ÿæç¤ºè¯ã€‚
        provider (Optional[str]): LLM æä¾›å•†ï¼ˆ"deepseek" æˆ– "openai"ï¼‰ï¼Œé»˜è®¤è¯»å–é…ç½®ã€‚

    Returns:
        Dict[str, Any]: å¯¹è¯ç»“æœï¼ŒåŒ…å«å›å¤å†…å®¹å’Œ Token ä½¿ç”¨ä¿¡æ¯ã€‚

    Raises:
        RuntimeError: å¦‚æœ API è°ƒç”¨å¤±è´¥ã€‚
        ValueError: å¦‚æœ provider ä¸åˆæ³•ã€‚

    Example:
        {
            "provider": "deepseek",
            "model": "deepseek-chat",
            "reply": "Python çš„è£…é¥°å™¨æ˜¯...",
            "usage": {"prompt_tokens": 150, "completion_tokens": 200}
        }
    """
    if provider is None:
        provider = get_config("llm.default_provider", "deepseek")

    if provider not in ("deepseek", "openai"):
        raise ValueError(f"ä¸æ”¯æŒçš„ LLM: {provider}ï¼Œè¯·ä½¿ç”¨ 'deepseek' æˆ– 'openai'")

    logger.info(f"è°ƒç”¨ LLM [{provider}]ï¼Œæ¶ˆæ¯é•¿åº¦: {len(message)}")

    try:
        api_key = _get_api_key(provider)

        if provider == "deepseek":
            base_url = get_config("llm.deepseek.base_url", "https://api.deepseek.com")
            model = get_config("llm.deepseek.model", "deepseek-chat")
        else:
            base_url = None
            model = get_config("llm.openai.model", "gpt-3.5-turbo")

        max_tokens = get_config(f"llm.{provider}.max_tokens", 2048)
        temperature = get_config(f"llm.{provider}.temperature", 0.7)
    except Exception as e:
        raise RuntimeError(f"è¯»å– LLM é…ç½®å¤±è´¥: {e}")

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)

        messages: List[Dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})

        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        reply = response.choices[0].message.content
        usage = {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
        }

        logger.info(f"LLM [{provider}] å›å¤æˆåŠŸï¼ŒToken: {usage}")

        return {
            "provider": provider,
            "model": model,
            "reply": reply,
            "usage": usage,
        }
    except Exception as e:
        raise RuntimeError(f"LLM è°ƒç”¨å¤±è´¥ [{provider}]: {e}")


def _get_api_key(provider: str) -> str:
    """ä»ç¯å¢ƒå˜é‡è·å– API Keyã€‚"""
    env_map = {
        "deepseek": "DEEPSEEK_API_KEY",
        "openai": "OPENAI_API_KEY",
    }
    env_var = env_map.get(provider, "")
    key = os.environ.get(env_var)

    if not key:
        try:
            from modules.YA_Secrets.secrets_parser import get_secret
            key = get_secret(env_var.lower())
        except Exception:
            pass

    if not key:
        raise RuntimeError(f"æœªæ‰¾åˆ° {provider} çš„ API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ {env_var}")
    return key
```

---

## A-6: å†™ç¬¬äºŒä¸ªæ ¸å¿ƒæ¨¡å— `core/weather_service.py`ï¼ˆ15åˆ†é’Ÿï¼‰

åˆ›å»ºæ–‡ä»¶ `core/weather_service.py`ï¼š

```python
"""
å¤©æ°”æŸ¥è¯¢æœåŠ¡æ¨¡å—

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- query_weather: ä½¿ç”¨ wttr.in å…è´¹ API æŸ¥è¯¢åŸå¸‚å¤©æ°”
"""

from typing import Dict, Any
import httpx
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("weather_service")


async def query_weather(city: str) -> Dict[str, Any]:
    """
    æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”ä¿¡æ¯ã€‚

    ä½¿ç”¨å…è´¹çš„ wttr.in APIï¼Œæ— éœ€ API Keyã€‚

    Args:
        city (str): åŸå¸‚åç§°ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼Œå¦‚ "åŒ—äº¬"ã€"London"ï¼‰ã€‚

    Returns:
        Dict[str, Any]: å¤©æ°”ä¿¡æ¯å­—å…¸ã€‚

    Raises:
        RuntimeError: å¦‚æœå¤©æ°” API è°ƒç”¨å¤±è´¥ã€‚

    Example:
        {
            "city": "åŒ—äº¬",
            "temperature": "25Â°C",
            "feels_like": "27Â°C",
            "weather": "æ™´",
            "humidity": "40%",
            "wind": "NE 12km/h"
        }
    """
    logger.info(f"æŸ¥è¯¢å¤©æ°”: {city}")

    try:
        url = f"https://wttr.in/{city}"
        params = {"format": "j1"}

        async with httpx.AsyncClient(timeout=15) as client:
            response = await client.get(url, params=params)
            response.raise_for_status()
            data = response.json()

        current = data.get("current_condition", [{}])[0]

        # å°è¯•è·å–ä¸­æ–‡å¤©æ°”æè¿°
        lang_zh = current.get("lang_zh", [])
        if lang_zh:
            weather_desc = lang_zh[0].get("value", "æœªçŸ¥")
        else:
            desc_list = current.get("weatherDesc", [{}])
            weather_desc = desc_list[0].get("value", "æœªçŸ¥") if desc_list else "æœªçŸ¥"

        result = {
            "city": city,
            "temperature": f"{current.get('temp_C', 'N/A')}Â°C",
            "feels_like": f"{current.get('FeelsLikeC', 'N/A')}Â°C",
            "weather": weather_desc,
            "humidity": f"{current.get('humidity', 'N/A')}%",
            "wind": f"{current.get('winddir16Point', '')} {current.get('windspeedKmph', '')}km/h",
            "visibility": f"{current.get('visibility', 'N/A')}km",
        }

        logger.info(f"å¤©æ°”æŸ¥è¯¢æˆåŠŸ: {city} - {weather_desc} {result['temperature']}")
        return result

    except httpx.HTTPStatusError as e:
        raise RuntimeError(f"å¤©æ°” API è¯·æ±‚å¤±è´¥ (HTTP {e.response.status_code}): {e}")
    except Exception as e:
        raise RuntimeError(f"å¤©æ°”æŸ¥è¯¢å¤±è´¥: {e}")
```

---

## A-7: å†™ç¬¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å— `core/translate_service.py`ï¼ˆ15åˆ†é’Ÿï¼‰

åˆ›å»ºæ–‡ä»¶ `core/translate_service.py`ï¼š

```python
"""
æ–‡æœ¬ç¿»è¯‘æœåŠ¡æ¨¡å—

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- translate_text: ä½¿ç”¨ MyMemory å…è´¹ API è¿›è¡Œå¤šè¯­è¨€ç¿»è¯‘
- get_supported_languages: è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
"""

from typing import Dict, Any
import httpx
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("translate_service")

LANGUAGE_MAP = {
    "ä¸­æ–‡": "zh-CN", "è‹±æ–‡": "en", "æ—¥æ–‡": "ja", "éŸ©æ–‡": "ko",
    "æ³•æ–‡": "fr", "å¾·æ–‡": "de", "è¥¿ç­ç‰™æ–‡": "es", "ä¿„æ–‡": "ru",
    "chinese": "zh-CN", "english": "en", "japanese": "ja", "korean": "ko",
    "french": "fr", "german": "de", "spanish": "es", "russian": "ru",
}


async def translate_text(
    text: str,
    target_lang: str = "è‹±æ–‡",
    source_lang: str = "auto",
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ MyMemory API è¿›è¡Œæ–‡æœ¬ç¿»è¯‘ï¼ˆå…è´¹ï¼Œæ— éœ€ Keyï¼‰ã€‚

    Args:
        text (str): è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚
        target_lang (str): ç›®æ ‡è¯­è¨€ï¼ˆå¦‚ "è‹±æ–‡"ã€"ä¸­æ–‡"ã€"ja"ï¼‰ï¼Œé»˜è®¤ "è‹±æ–‡"ã€‚
        source_lang (str): æºè¯­è¨€ï¼Œé»˜è®¤ "auto" è‡ªåŠ¨æ£€æµ‹ã€‚

    Returns:
        Dict[str, Any]: ç¿»è¯‘ç»“æœã€‚

    Raises:
        RuntimeError: å¦‚æœç¿»è¯‘å¤±è´¥ã€‚

    Example:
        {
            "original": "ä½ å¥½ä¸–ç•Œ",
            "translated": "Hello World",
            "source_lang": "zh-CN",
            "target_lang": "en"
        }
    """
    logger.info(f"ç¿»è¯‘: '{text[:50]}' -> {target_lang}")

    target_code = LANGUAGE_MAP.get(target_lang, target_lang)
    source_code = LANGUAGE_MAP.get(source_lang, source_lang) if source_lang != "auto" else "autodetect"

    try:
        params = {
            "q": text,
            "langpair": f"{source_code}|{target_code}",
        }

        async with httpx.AsyncClient(timeout=15) as client:
            response = await client.get("https://api.mymemory.translated.net/get", params=params)
            response.raise_for_status()
            data = response.json()

        if data.get("responseStatus") != 200:
            raise RuntimeError(f"ç¿»è¯‘ API é”™è¯¯: {data.get('responseDetails', 'æœªçŸ¥')}")

        return {
            "original": text,
            "translated": data["responseData"]["translatedText"],
            "source_lang": source_code,
            "target_lang": target_code,
        }
    except RuntimeError:
        raise
    except Exception as e:
        raise RuntimeError(f"ç¿»è¯‘å¤±è´¥: {e}")


def get_supported_languages() -> Dict[str, str]:
    """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ã€‚"""
    return LANGUAGE_MAP.copy()
```

---

## A-8: å†™ MCP Toolsï¼ˆ30åˆ†é’Ÿï¼‰

### åˆ›å»º `tools/chat_tool.py`

```python
"""
æ™ºèƒ½å¯¹è¯å·¥å…·ï¼ŒåŒ…æ‹¬ï¼š
- smart_chat: è°ƒç”¨ LLM è¿›è¡Œå¯¹è¯
"""

from typing import Any, Dict, Optional
from tools import YA_MCPServer_Tool


@YA_MCPServer_Tool(
    name="smart_chat",
    title="Smart Chat",
    description="è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½å¯¹è¯ï¼Œæ”¯æŒ DeepSeek å’Œ OpenAI",
)
async def smart_chat(
    message: str,
    provider: str = "deepseek",
    system_prompt: Optional[str] = None,
) -> Dict[str, Any]:
    """è°ƒç”¨ LLM è¿›è¡Œæ™ºèƒ½å¯¹è¯ã€‚

    Args:
        message (str): ç”¨æˆ·æ¶ˆæ¯ã€‚
        provider (str): LLM æä¾›å•†ï¼ˆ"deepseek" æˆ– "openai"ï¼‰ï¼Œé»˜è®¤ "deepseek"ã€‚
        system_prompt (Optional[str]): ç³»ç»Ÿæç¤ºè¯ã€‚

    Returns:
        Dict[str, Any]: AI å›å¤å’Œ Token ä½¿ç”¨ä¿¡æ¯ã€‚
    """
    try:
        from core.llm_service import chat_with_llm
    except ImportError as e:
        raise RuntimeError(f"æ— æ³•å¯¼å…¥ LLM æ¨¡å—: {e}")

    return await chat_with_llm(message=message, system_prompt=system_prompt, provider=provider)
```

### åˆ›å»º `tools/weather_tool.py`

```python
"""
å¤©æ°”æŸ¥è¯¢å·¥å…·ï¼ŒåŒ…æ‹¬ï¼š
- weather_query: æŸ¥è¯¢åŸå¸‚å¤©æ°”
"""

from typing import Any, Dict
from tools import YA_MCPServer_Tool


@YA_MCPServer_Tool(
    name="weather_query",
    title="Weather Query",
    description="æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”ä¿¡æ¯ï¼Œæ”¯æŒä¸­è‹±æ–‡åŸå¸‚åï¼Œå…è´¹æ— éœ€ Key",
)
async def weather_query(city: str) -> Dict[str, Any]:
    """æŸ¥è¯¢åŸå¸‚å¤©æ°”ã€‚

    Args:
        city (str): åŸå¸‚åç§°ï¼ˆå¦‚ "åŒ—äº¬"ã€"ä¸Šæµ·"ã€"London"ï¼‰ã€‚

    Returns:
        Dict[str, Any]: æ¸©åº¦ã€å¤©æ°”ã€æ¹¿åº¦ã€é£åŠ›ç­‰ä¿¡æ¯ã€‚
    """
    try:
        from core.weather_service import query_weather
    except ImportError as e:
        raise RuntimeError(f"æ— æ³•å¯¼å…¥å¤©æ°”æ¨¡å—: {e}")

    return await query_weather(city=city)
```

### åˆ›å»º `tools/translate_tool.py`

```python
"""
æ–‡æœ¬ç¿»è¯‘å·¥å…·ï¼ŒåŒ…æ‹¬ï¼š
- text_translate: å¤šè¯­è¨€ç¿»è¯‘
- get_supported_languages: è·å–æ”¯æŒçš„è¯­è¨€
"""

from typing import Any, Dict
from tools import YA_MCPServer_Tool


@YA_MCPServer_Tool(
    name="text_translate",
    title="Text Translate",
    description="å¤šè¯­è¨€æ–‡æœ¬ç¿»è¯‘ï¼Œæ”¯æŒä¸­è‹±æ—¥éŸ©æ³•å¾·è¥¿ä¿„ç­‰è¯­è¨€äº’è¯‘ï¼Œå…è´¹æ— éœ€ Key",
)
async def text_translate(
    text: str,
    target_lang: str = "è‹±æ–‡",
    source_lang: str = "auto",
) -> Dict[str, Any]:
    """ç¿»è¯‘æ–‡æœ¬ã€‚

    Args:
        text (str): è¦ç¿»è¯‘çš„æ–‡æœ¬ã€‚
        target_lang (str): ç›®æ ‡è¯­è¨€ï¼Œé»˜è®¤ "è‹±æ–‡"ã€‚
        source_lang (str): æºè¯­è¨€ï¼Œé»˜è®¤ "auto"ã€‚

    Returns:
        Dict[str, Any]: ç¿»è¯‘ç»“æœã€‚
    """
    try:
        from core.translate_service import translate_text
    except ImportError as e:
        raise RuntimeError(f"æ— æ³•å¯¼å…¥ç¿»è¯‘æ¨¡å—: {e}")

    return await translate_text(text=text, target_lang=target_lang, source_lang=source_lang)


@YA_MCPServer_Tool(
    name="get_supported_languages",
    title="Supported Languages",
    description="è·å–ç¿»è¯‘æ”¯æŒçš„æ‰€æœ‰è¯­è¨€åˆ—è¡¨",
)
async def get_languages() -> Dict[str, str]:
    """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ã€‚"""
    try:
        from core.translate_service import get_supported_languages
    except ImportError as e:
        raise RuntimeError(f"æ— æ³•å¯¼å…¥ç¿»è¯‘æ¨¡å—: {e}")

    return get_supported_languages()
```

---

## A-9: è®¾ç½® API Key & æµ‹è¯•ï¼ˆ15åˆ†é’Ÿï¼‰

```powershell
# è®¾ç½® DeepSeek Keyï¼ˆå›½å†…ç›´è¿ï¼Œæ¨èï¼‰
$env:DEEPSEEK_API_KEY="ä½ çš„Key"

# å¯åŠ¨æœåŠ¡å™¨
uv run server.py
```

### ç”¨ MCP Inspector æµ‹è¯•

æ–°çª—å£ï¼š
```powershell
npx @anthropic/mcp-inspector
```

æµè§ˆå™¨ä¸­è¿æ¥ `http://127.0.0.1:12345/sse`ï¼Œæµ‹è¯•ï¼š
- `weather_query` â†’ city: "åŒ—äº¬" âœ…
- `text_translate` â†’ text: "ä½ å¥½", target_lang: "è‹±æ–‡" âœ…
- `smart_chat` â†’ message: "ä½ å¥½" âœ…

---

## A-10: Git æäº¤é˜¶æ®µ Aï¼ˆ5åˆ†é’Ÿï¼‰

```powershell
git add .
git commit -m "feat: Phase A - LLM chat, weather query, translation tools"
```

---

## âœ… é˜¶æ®µ A å®Œæˆï¼

æ­¤æ—¶ä½ å·²ç»ï¼š
- [x] ç†è§£äº† MCP Server æ¨¡æ¿çš„å®Œæ•´å¼€å‘æµç¨‹
- [x] ä¼šå†™ `core/` æ ¸å¿ƒæ¨¡å—
- [x] ä¼šå†™ `tools/` MCP å·¥å…·
- [x] ä¼šç”¨ MCP Inspector æµ‹è¯•
- [x] æœ‰äº†å¯å¤ç”¨çš„ `llm_service.py`

**è€—æ—¶çº¦ 2-3 å°æ—¶ã€‚**

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é˜¶æ®µ Bï¼šè¿›é˜¶ä¸ºä¸­ç­‰é¢˜ï¼ˆç¬¬ 21 é¢˜ - çŸ¥è¯†ç®¡ç†æ™ºèƒ½ä½“ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

> **ç›®æ ‡ï¼š** åœ¨é˜¶æ®µ A çš„åŸºç¡€ä¸Šï¼Œæ–°å¢å‘é‡æ•°æ®åº“ + RAGï¼Œå‡çº§ä¸ºä¸­ç­‰éš¾åº¦
> **æ–°å¢å·¥ä½œé‡ï¼š** 4-5 å°æ—¶
> **å¤ç”¨é˜¶æ®µ A çš„ï¼š** llm_service.pyã€config.yamlã€Gitã€ç¯å¢ƒã€æ‰€æœ‰å·²æœ‰ Tools

---

## B-1: æ–°å¢ä¾èµ–ï¼ˆ5åˆ†é’Ÿï¼‰

ä¿®æ”¹ `pyproject.toml`ï¼Œåœ¨ dependencies é‡Œ**æ–°å¢ä¸€è¡Œ**ï¼š

```toml
dependencies = [
    # ... ä¿ç•™é˜¶æ®µ A çš„æ‰€æœ‰ä¾èµ– ...
    "chromadb>=0.5.0",
]
```

ç„¶åï¼š
```powershell
uv sync
# éªŒè¯
python -c "import chromadb; print('ChromaDB OK')"
```

> ChromaDB é¦–æ¬¡å¯¼å…¥ä¼šè‡ªåŠ¨ä¸‹è½½åµŒå…¥æ¨¡å‹ï¼ˆ~80MBï¼‰ï¼Œéœ€è¦ç½‘ç»œã€‚

---

## B-2: æ›´æ–° `config.yaml`ï¼ˆ5åˆ†é’Ÿï¼‰

åœ¨ `config.yaml` æœ«å°¾**è¿½åŠ **çŸ¥è¯†åº“é…ç½®ï¼ˆä¿ç•™é˜¶æ®µ A å·²æœ‰çš„é…ç½®ä¸åŠ¨ï¼‰ï¼š

```yaml
# ===== é˜¶æ®µ B æ–°å¢ï¼šçŸ¥è¯†ç®¡ç†é…ç½® =====
knowledge:
  chromadb:
    persist_directory: "./data/chromadb"
    collection_name: "knowledge_base"
  chunking:
    chunk_size: 500
    chunk_overlap: 50
  retrieval:
    top_k: 5
    min_relevance: 0.3
```

åœ¨ `.gitignore` ä¸­æ·»åŠ ï¼š
```
data/
```

---

## B-3: æ–°å»º `core/document_processor.py`ï¼ˆ20åˆ†é’Ÿï¼‰

æ–‡æœ¬åˆ†å—æ¨¡å—ï¼ŒæŠŠé•¿æ–‡æœ¬åˆ‡æˆé€‚åˆå‘é‡æ•°æ®åº“å­˜å‚¨çš„å°ç‰‡æ®µã€‚

```python
"""
æ–‡æ¡£å¤„ç†æ¨¡å—

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- split_text: å°†é•¿æ–‡æœ¬æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µï¼ˆæ”¯æŒé‡å ï¼‰
"""

from typing import List
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("document_processor")


def split_text(
    text: str,
    chunk_size: int = 500,
    chunk_overlap: int = 50,
) -> List[str]:
    """
    å°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µã€‚

    åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰è‡ªç„¶æ–­ç‚¹å¤„ä¼˜å…ˆåˆ‡åˆ†ã€‚

    Args:
        text (str): è¦åˆ‡åˆ†çš„æ–‡æœ¬ã€‚
        chunk_size (int): æ¯ä¸ªç‰‡æ®µæœ€å¤§å­—ç¬¦æ•°ï¼Œé»˜è®¤ 500ã€‚
        chunk_overlap (int): ç›¸é‚»ç‰‡æ®µé‡å å­—ç¬¦æ•°ï¼Œé»˜è®¤ 50ã€‚

    Returns:
        List[str]: åˆ‡åˆ†åçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨ã€‚

    Raises:
        ValueError: å¦‚æœå‚æ•°ä¸åˆæ³•ã€‚

    Example:
        >>> split_text("ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬...", chunk_size=100)
        ["ä¸€æ®µå¾ˆé•¿çš„...", "...çš„æ–‡æœ¬æ¥ä¸‹æ¥..."]
    """
    if chunk_size <= 0:
        raise ValueError(f"chunk_size å¿…é¡»å¤§äº 0ï¼Œå½“å‰: {chunk_size}")
    if chunk_overlap < 0 or chunk_overlap >= chunk_size:
        raise ValueError(f"chunk_overlap ({chunk_overlap}) å¿…é¡»åœ¨ [0, {chunk_size}) èŒƒå›´å†…")

    text = text.strip()
    if not text:
        return []
    if len(text) <= chunk_size:
        return [text]

    break_chars = ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ". ", "! ", "? ", "ï¼›"]
    chunks: List[str] = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        if end >= len(text):
            chunks.append(text[start:].strip())
            break

        # åœ¨ååŠæ®µå¯»æ‰¾è‡ªç„¶æ–­ç‚¹
        best_break = -1
        for bc in break_chars:
            pos = text.rfind(bc, start + chunk_size // 2, end)
            if pos > best_break:
                best_break = pos + len(bc)

        if best_break > start:
            end = best_break

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        start = end - chunk_overlap

    logger.debug(f"æ–‡æœ¬åˆ†å—: åŸå§‹={len(text)}å­—, ç‰‡æ®µæ•°={len(chunks)}")
    return chunks
```

---

## B-4: æ–°å»º `core/knowledge_store.py`ï¼ˆ40åˆ†é’Ÿï¼‰â­ æ ¸å¿ƒ

å‘é‡æ•°æ®åº“æ“ä½œå°è£…ï¼Œè¿™æ˜¯é˜¶æ®µ B æœ€é‡è¦çš„æ¨¡å—ã€‚

```python
"""
çŸ¥è¯†å­˜å‚¨æ¨¡å— - ChromaDB å‘é‡æ•°æ®åº“å°è£…

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- add_knowledge: æ·»åŠ çŸ¥è¯†åˆ°å‘é‡æ•°æ®åº“
- search_knowledge: è¯­ä¹‰æœç´¢çŸ¥è¯†
- list_knowledge: åˆ—å‡ºçŸ¥è¯†æ¡ç›®
- delete_knowledge: åˆ é™¤çŸ¥è¯†
- get_stats: è·å–ç»Ÿè®¡ä¿¡æ¯
"""

import uuid
from typing import Dict, Any, Optional
import chromadb
from modules.YA_Common.utils.config import get_config
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("knowledge_store")

_client: Optional[chromadb.PersistentClient] = None
_collection = None


def _get_client() -> chromadb.PersistentClient:
    """è·å– ChromaDB å®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼‰"""
    global _client
    if _client is None:
        path = get_config("knowledge.chromadb.persist_directory", "./data/chromadb")
        logger.info(f"åˆå§‹åŒ– ChromaDB: {path}")
        _client = chromadb.PersistentClient(path=path)
    return _client


def get_collection():
    """è·å–çŸ¥è¯†åº“é›†åˆ"""
    global _collection
    if _collection is None:
        name = get_config("knowledge.chromadb.collection_name", "knowledge_base")
        _collection = _get_client().get_or_create_collection(name=name)
        logger.info(f"é›†åˆ '{name}' å·²åŠ è½½ï¼Œå½“å‰ {_collection.count()} æ¡")
    return _collection


async def add_knowledge(
    content: str,
    title: str = "",
    tags: str = "",
    source: str = "",
) -> Dict[str, Any]:
    """
    æ·»åŠ çŸ¥è¯†åˆ°å‘é‡æ•°æ®åº“ï¼ˆè‡ªåŠ¨åˆ†å— + è‡ªåŠ¨å‘é‡åŒ–ï¼‰ã€‚

    Args:
        content (str): çŸ¥è¯†å†…å®¹æ–‡æœ¬ã€‚
        title (str): æ ‡é¢˜ã€‚
        tags (str): æ ‡ç­¾ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚ "python,ç¼–ç¨‹"ï¼‰ã€‚
        source (str): æ¥æºï¼ˆå¦‚ "è¯¾ä»¶"ã€"ç¬”è®°"ï¼‰ã€‚

    Returns:
        Dict[str, Any]: æ·»åŠ ç»“æœï¼ŒåŒ…å« ID å’Œåˆ†å—æ•°ã€‚

    Raises:
        RuntimeError: å¦‚æœæ·»åŠ å¤±è´¥ã€‚

    Example:
        {"id": "kb_a1b2c3d4", "title": "Pythonè£…é¥°å™¨", "chunks_count": 2, "message": "çŸ¥è¯†æ·»åŠ æˆåŠŸ"}
    """
    logger.info(f"æ·»åŠ çŸ¥è¯†: title='{title}', len={len(content)}")

    try:
        from core.document_processor import split_text
    except ImportError as e:
        raise RuntimeError(f"æ— æ³•å¯¼å…¥æ–‡æ¡£å¤„ç†æ¨¡å—: {e}")

    try:
        collection = get_collection()
        chunk_size = get_config("knowledge.chunking.chunk_size", 500)
        chunk_overlap = get_config("knowledge.chunking.chunk_overlap", 50)
        chunks = split_text(content, chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        base_id = f"kb_{uuid.uuid4().hex[:8]}"
        tag_list = [t.strip() for t in tags.split(",") if t.strip()] if tags else []

        ids, documents, metadatas = [], [], []
        for i, chunk in enumerate(chunks):
            ids.append(f"{base_id}_chunk{i}")
            documents.append(chunk)
            metadatas.append({
                "title": title,
                "tags": ",".join(tag_list),
                "source": source,
                "base_id": base_id,
                "chunk_index": i,
                "total_chunks": len(chunks),
            })

        collection.add(ids=ids, documents=documents, metadatas=metadatas)
        logger.info(f"æ·»åŠ æˆåŠŸ: {base_id}, {len(chunks)} ä¸ªç‰‡æ®µ")

        return {
            "id": base_id,
            "title": title,
            "tags": tag_list,
            "chunks_count": len(chunks),
            "message": "çŸ¥è¯†æ·»åŠ æˆåŠŸ",
        }
    except Exception as e:
        raise RuntimeError(f"æ·»åŠ çŸ¥è¯†å¤±è´¥: {e}")


async def search_knowledge(
    query: str,
    top_k: int = 5,
    tag_filter: str = "",
) -> Dict[str, Any]:
    """
    è¯­ä¹‰æœç´¢çŸ¥è¯†åº“ã€‚

    Args:
        query (str): æœç´¢æŸ¥è¯¢ã€‚
        top_k (int): è¿”å›å‰ K æ¡ç»“æœã€‚
        tag_filter (str): å¯é€‰æ ‡ç­¾è¿‡æ»¤ã€‚

    Returns:
        Dict[str, Any]: æœç´¢ç»“æœã€‚

    Example:
        {"query": "è£…é¥°å™¨", "total_results": 2, "results": [{"content": "...", "relevance": 0.85}]}
    """
    logger.info(f"æœç´¢: '{query}', top_k={top_k}")

    try:
        collection = get_collection()
        n = min(top_k, get_config("knowledge.retrieval.top_k", 5), max(collection.count(), 1))

        if collection.count() == 0:
            return {"query": query, "total_results": 0, "results": [], "message": "çŸ¥è¯†åº“ä¸ºç©º"}

        query_params = {"query_texts": [query], "n_results": n}
        if tag_filter:
            query_params["where"] = {"tags": {"$contains": tag_filter}}

        results = collection.query(**query_params)

        formatted = []
        if results and results["ids"] and results["ids"][0]:
            for i in range(len(results["ids"][0])):
                dist = results["distances"][0][i] if results.get("distances") else 0
                formatted.append({
                    "id": results["ids"][0][i],
                    "content": results["documents"][0][i],
                    "title": results["metadatas"][0][i].get("title", ""),
                    "tags": results["metadatas"][0][i].get("tags", ""),
                    "source": results["metadatas"][0][i].get("source", ""),
                    "relevance": round(1 - dist, 4),
                })

        logger.info(f"æœç´¢è¿”å› {len(formatted)} æ¡")
        return {"query": query, "total_results": len(formatted), "results": formatted}
    except Exception as e:
        raise RuntimeError(f"æœç´¢å¤±è´¥: {e}")


async def list_knowledge(tag_filter: str = "", limit: int = 20) -> Dict[str, Any]:
    """
    åˆ—å‡ºçŸ¥è¯†æ¡ç›®ã€‚

    Args:
        tag_filter (str): æ ‡ç­¾è¿‡æ»¤ã€‚
        limit (int): æœ€å¤§è¿”å›æ•°ã€‚

    Returns:
        Dict[str, Any]: çŸ¥è¯†åˆ—è¡¨ã€‚
    """
    try:
        collection = get_collection()
        get_params = {"limit": limit}
        if tag_filter:
            get_params["where"] = {"tags": {"$contains": tag_filter}}

        results = collection.get(**get_params)

        seen = {}
        for i in range(len(results["ids"])):
            meta = results["metadatas"][i]
            bid = meta.get("base_id", results["ids"][i])
            if bid not in seen:
                preview = results["documents"][i]
                seen[bid] = {
                    "id": bid,
                    "title": meta.get("title", "æœªå‘½å"),
                    "tags": meta.get("tags", ""),
                    "source": meta.get("source", ""),
                    "total_chunks": meta.get("total_chunks", 1),
                    "preview": preview[:100] + "..." if len(preview) > 100 else preview,
                }

        return {"total_items": len(seen), "total_chunks": collection.count(), "items": list(seen.values())}
    except Exception as e:
        raise RuntimeError(f"åˆ—å‡ºçŸ¥è¯†å¤±è´¥: {e}")


async def delete_knowledge(knowledge_id: str) -> Dict[str, str]:
    """
    åˆ é™¤çŸ¥è¯†ã€‚

    Args:
        knowledge_id (str): çŸ¥è¯† base_idã€‚

    Returns:
        Dict[str, str]: åˆ é™¤ç»“æœã€‚
    """
    try:
        collection = get_collection()
        results = collection.get(where={"base_id": knowledge_id})

        if not results["ids"]:
            return {"message": f"æœªæ‰¾åˆ° ID '{knowledge_id}'"}

        collection.delete(ids=results["ids"])
        return {"message": f"å·²åˆ é™¤ '{knowledge_id}'ï¼Œå…± {len(results['ids'])} ä¸ªç‰‡æ®µ"}
    except Exception as e:
        raise RuntimeError(f"åˆ é™¤å¤±è´¥: {e}")


async def get_stats() -> Dict[str, Any]:
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ã€‚"""
    try:
        collection = get_collection()
        total = collection.count()
        all_data = collection.get() if total > 0 else {"metadatas": []}

        base_ids, tag_counts, source_counts = set(), {}, {}
        for meta in all_data.get("metadatas", []):
            base_ids.add(meta.get("base_id", "?"))
            for t in meta.get("tags", "").split(","):
                t = t.strip()
                if t:
                    tag_counts[t] = tag_counts.get(t, 0) + 1
            s = meta.get("source", "")
            if s:
                source_counts[s] = source_counts.get(s, 0) + 1

        return {"total_items": len(base_ids), "total_chunks": total, "tags": tag_counts, "sources": source_counts}
    except Exception as e:
        raise RuntimeError(f"ç»Ÿè®¡å¤±è´¥: {e}")
```

---

## B-5: æ–°å»º `core/rag_service.py`ï¼ˆ30åˆ†é’Ÿï¼‰â­ RAG æ ¸å¿ƒ

```python
"""
RAG (Retrieval-Augmented Generation) æœåŠ¡æ¨¡å—

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- ask_knowledge: æ£€ç´¢çŸ¥è¯†åº“ + LLM ç”Ÿæˆå›ç­”
"""

from typing import Dict, Any, Optional
from modules.YA_Common.utils.config import get_config
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("rag_service")

RAG_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„çŸ¥è¯†å†…å®¹å›ç­”é—®é¢˜ã€‚

## è§„åˆ™ï¼š
1. åªåŸºäºæä¾›çš„çŸ¥è¯†å†…å®¹å›ç­”ï¼Œä¸ç¼–é€ 
2. çŸ¥è¯†ä¸è¶³æ—¶æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. æ ‡æ³¨ä¿¡æ¯æ¥æº
4. æ¡ç†æ¸…æ™°

## æ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼š
{context}

å¦‚æœä»¥ä¸ŠçŸ¥è¯†ä¸åŒ…å«ç­”æ¡ˆï¼Œå›ç­”"æ ¹æ®ç°æœ‰çŸ¥è¯†åº“æš‚æ— ç›¸å…³ä¿¡æ¯ï¼Œå»ºè®®æ·»åŠ ç›¸å…³çŸ¥è¯†åå†æ¬¡æé—®ã€‚"
"""


async def ask_knowledge(
    question: str,
    top_k: int = 5,
    provider: Optional[str] = None,
) -> Dict[str, Any]:
    """
    åŸºäºçŸ¥è¯†åº“çš„ RAG æ™ºèƒ½é—®ç­”ã€‚

    æµç¨‹: è¯­ä¹‰æ£€ç´¢çŸ¥è¯†ç‰‡æ®µ â†’ æ‹¼æ¥ä¸ºä¸Šä¸‹æ–‡ â†’ LLM ç”Ÿæˆå›ç­”

    Args:
        question (str): ç”¨æˆ·é—®é¢˜ã€‚
        top_k (int): æ£€ç´¢ç‰‡æ®µæ•°ï¼Œé»˜è®¤ 5ã€‚
        provider (Optional[str]): LLM æä¾›å•†ã€‚

    Returns:
        Dict[str, Any]: å›ç­” + å¼•ç”¨æ¥æºã€‚

    Raises:
        RuntimeError: å¦‚æœæµç¨‹å¤±è´¥ã€‚

    Example:
        {
            "question": "Pythonè£…é¥°å™¨æ€ä¹ˆç”¨ï¼Ÿ",
            "answer": "æ ¹æ®çŸ¥è¯†åº“...",
            "sources": [{"title": "Pythonè£…é¥°å™¨", "relevance": 0.85}],
            "context_chunks_used": 3
        }
    """
    logger.info(f"RAG é—®ç­”: '{question[:50]}'")

    try:
        from core.knowledge_store import search_knowledge
    except ImportError as e:
        raise RuntimeError(f"æ— æ³•å¯¼å…¥çŸ¥è¯†æ¨¡å—: {e}")

    try:
        search_results = await search_knowledge(query=question, top_k=top_k)
    except Exception as e:
        raise RuntimeError(f"çŸ¥è¯†æ£€ç´¢å¤±è´¥: {e}")

    results = search_results.get("results", [])
    if not results:
        return {
            "question": question,
            "answer": "çŸ¥è¯†åº“ä¸­æš‚æ— ç›¸å…³å†…å®¹ï¼Œè¯·å…ˆç”¨ add_knowledge æ·»åŠ çŸ¥è¯†ã€‚",
            "sources": [],
            "context_chunks_used": 0,
        }

    # è¿‡æ»¤ä½ç›¸å…³åº¦
    min_rel = get_config("knowledge.retrieval.min_relevance", 0.3)
    filtered = [r for r in results if r.get("relevance", 0) >= min_rel] or results[:2]

    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts, sources = [], []
    for i, r in enumerate(filtered):
        context_parts.append(
            f"ã€çŸ¥è¯†{i+1}ã€‘(æ¥æº: {r.get('title', 'æœªçŸ¥')}, ç›¸å…³åº¦: {r.get('relevance', 0):.2f})\n{r['content']}"
        )
        sources.append({"title": r.get("title", "æœªçŸ¥"), "relevance": r.get("relevance", 0)})

    system_prompt = RAG_SYSTEM_PROMPT.format(context="\n\n---\n\n".join(context_parts))

    try:
        from core.llm_service import chat_with_llm
    except ImportError as e:
        raise RuntimeError(f"æ— æ³•å¯¼å…¥ LLM æ¨¡å—: {e}")

    try:
        llm_resp = await chat_with_llm(message=question, system_prompt=system_prompt, provider=provider)
    except Exception as e:
        raise RuntimeError(f"LLM ç”Ÿæˆå¤±è´¥: {e}")

    logger.info(f"RAG å®Œæˆï¼Œä½¿ç”¨ {len(filtered)} ä¸ªç‰‡æ®µ")

    return {
        "question": question,
        "answer": llm_resp["reply"],
        "sources": sources,
        "llm_provider": llm_resp["provider"],
        "context_chunks_used": len(filtered),
        "token_usage": llm_resp["usage"],
    }
```

---

## B-6: æ–°å»ºçŸ¥è¯†ç®¡ç† Toolsï¼ˆ30åˆ†é’Ÿï¼‰

### åˆ›å»º `tools/knowledge_tool.py`

```python
"""
çŸ¥è¯†ç®¡ç†å·¥å…·ï¼ŒåŒ…æ‹¬ï¼š
- add_knowledge: æ·»åŠ çŸ¥è¯†
- search_knowledge: è¯­ä¹‰æœç´¢
- list_knowledge: åˆ—å‡ºçŸ¥è¯†
- delete_knowledge: åˆ é™¤çŸ¥è¯†
"""

from typing import Any, Dict
from tools import YA_MCPServer_Tool


@YA_MCPServer_Tool(
    name="add_knowledge",
    title="Add Knowledge",
    description="æ·»åŠ çŸ¥è¯†åˆ°ä¸ªäººçŸ¥è¯†åº“ï¼Œæ”¯æŒç¬”è®°ã€æ–‡æ¡£ã€è¯¾ä»¶ç­‰ï¼Œè‡ªåŠ¨åˆ†å—å»ºç«‹å‘é‡ç´¢å¼•",
)
async def add_knowledge(
    content: str, title: str = "", tags: str = "", source: str = "",
) -> Dict[str, Any]:
    """æ·»åŠ çŸ¥è¯†åˆ°å‘é‡æ•°æ®åº“ã€‚

    Args:
        content (str): çŸ¥è¯†å†…å®¹ã€‚
        title (str): æ ‡é¢˜ã€‚
        tags (str): æ ‡ç­¾ï¼ˆé€—å·åˆ†éš”ï¼‰ã€‚
        source (str): æ¥æºã€‚
    Returns:
        Dict[str, Any]: æ·»åŠ ç»“æœã€‚
    """
    try:
        from core.knowledge_store import add_knowledge as _add
    except ImportError as e:
        raise RuntimeError(f"å¯¼å…¥å¤±è´¥: {e}")
    return await _add(content=content, title=title, tags=tags, source=source)


@YA_MCPServer_Tool(
    name="search_knowledge",
    title="Search Knowledge",
    description="è¯­ä¹‰æœç´¢çŸ¥è¯†åº“ï¼ŒåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è€Œéå…³é”®è¯åŒ¹é…",
)
async def search_knowledge(
    query: str, top_k: int = 5, tag_filter: str = "",
) -> Dict[str, Any]:
    """è¯­ä¹‰æœç´¢çŸ¥è¯†åº“ã€‚

    Args:
        query (str): æœç´¢å†…å®¹ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰ã€‚
        top_k (int): è¿”å›å‰ K æ¡ã€‚
        tag_filter (str): æ ‡ç­¾è¿‡æ»¤ã€‚
    Returns:
        Dict[str, Any]: æœç´¢ç»“æœã€‚
    """
    try:
        from core.knowledge_store import search_knowledge as _search
    except ImportError as e:
        raise RuntimeError(f"å¯¼å…¥å¤±è´¥: {e}")
    return await _search(query=query, top_k=top_k, tag_filter=tag_filter)


@YA_MCPServer_Tool(
    name="list_knowledge",
    title="List Knowledge",
    description="åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰çŸ¥è¯†æ¡ç›®",
)
async def list_knowledge(tag_filter: str = "", limit: int = 20) -> Dict[str, Any]:
    """åˆ—å‡ºçŸ¥è¯†ã€‚

    Args:
        tag_filter (str): æ ‡ç­¾è¿‡æ»¤ã€‚
        limit (int): æœ€å¤§æ•°é‡ã€‚
    Returns:
        Dict[str, Any]: çŸ¥è¯†åˆ—è¡¨ã€‚
    """
    try:
        from core.knowledge_store import list_knowledge as _list
    except ImportError as e:
        raise RuntimeError(f"å¯¼å…¥å¤±è´¥: {e}")
    return await _list(tag_filter=tag_filter, limit=limit)


@YA_MCPServer_Tool(
    name="delete_knowledge",
    title="Delete Knowledge",
    description="åˆ é™¤æŒ‡å®šçš„çŸ¥è¯†æ¡ç›®",
)
async def delete_knowledge(knowledge_id: str) -> Dict[str, str]:
    """åˆ é™¤çŸ¥è¯†ã€‚

    Args:
        knowledge_id (str): çŸ¥è¯† IDã€‚
    Returns:
        Dict[str, str]: åˆ é™¤ç»“æœã€‚
    """
    try:
        from core.knowledge_store import delete_knowledge as _del
    except ImportError as e:
        raise RuntimeError(f"å¯¼å…¥å¤±è´¥: {e}")
    return await _del(knowledge_id=knowledge_id)
```

### åˆ›å»º `tools/qa_tool.py`

```python
"""
RAG æ™ºèƒ½é—®ç­”å·¥å…·ï¼ŒåŒ…æ‹¬ï¼š
- ask_knowledge: åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”
- knowledge_stats: çŸ¥è¯†åº“ç»Ÿè®¡
"""

from typing import Any, Dict, Optional
from tools import YA_MCPServer_Tool


@YA_MCPServer_Tool(
    name="ask_knowledge",
    title="Ask Knowledge (RAG)",
    description="åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”ï¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³çŸ¥è¯† + å¤§æ¨¡å‹ç”Ÿæˆå›ç­” + æ ‡æ³¨æ¥æº",
)
async def ask_knowledge(
    question: str, top_k: int = 5, provider: Optional[str] = None,
) -> Dict[str, Any]:
    """RAG æ™ºèƒ½é—®ç­”ã€‚

    Args:
        question (str): ä½ çš„é—®é¢˜ã€‚
        top_k (int): æ£€ç´¢ç‰‡æ®µæ•°ã€‚
        provider (Optional[str]): LLM æä¾›å•†ã€‚
    Returns:
        Dict[str, Any]: å›ç­” + å¼•ç”¨æ¥æºã€‚
    """
    try:
        from core.rag_service import ask_knowledge as _ask
    except ImportError as e:
        raise RuntimeError(f"å¯¼å…¥å¤±è´¥: {e}")
    return await _ask(question=question, top_k=top_k, provider=provider)


@YA_MCPServer_Tool(
    name="knowledge_stats",
    title="Knowledge Stats",
    description="è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯ï¼šæ¡ç›®æ•°ã€æ ‡ç­¾åˆ†å¸ƒã€æ¥æºåˆ†å¸ƒ",
)
async def knowledge_stats() -> Dict[str, Any]:
    """è·å–ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    try:
        from core.knowledge_store import get_stats
    except ImportError as e:
        raise RuntimeError(f"å¯¼å…¥å¤±è´¥: {e}")
    return await get_stats()
```

---

## B-7: æ–°å»º Resources & Promptsï¼ˆ20åˆ†é’Ÿï¼‰

### åˆ›å»º `resources/knowledge_resource.py`

```python
"""
çŸ¥è¯†åº“ä½¿ç”¨æŒ‡å—èµ„æº
"""

from resources import YA_MCPServer_Resource


@YA_MCPServer_Resource(
    "docs://knowledge-guide",
    name="knowledge_guide",
    title="Knowledge Guide",
    description="çŸ¥è¯†ç®¡ç†æ™ºèƒ½ä½“ä½¿ç”¨æŒ‡å—",
    mime_type="text/markdown",
)
def get_knowledge_guide() -> str:
    """è¿”å›ä½¿ç”¨æŒ‡å—"""
    return """
# ğŸ“š çŸ¥è¯†ç®¡ç†æ™ºèƒ½ä½“ä½¿ç”¨æŒ‡å—

## æ·»åŠ çŸ¥è¯† â†’ add_knowledge
- content: å†…å®¹ï¼ˆå¿…å¡«ï¼‰| title: æ ‡é¢˜ | tags: æ ‡ç­¾ | source: æ¥æº

## è¯­ä¹‰æœç´¢ â†’ search_knowledge
- query: æœç´¢å†…å®¹ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰

## æ™ºèƒ½é—®ç­” â†’ ask_knowledge
- question: ä½ çš„é—®é¢˜ï¼ˆè‡ªåŠ¨æ£€ç´¢+LLMå›ç­”ï¼‰

## ç®¡ç† â†’ list_knowledge / delete_knowledge / knowledge_stats
"""
```

### åˆ›å»º `prompts/knowledge_prompt.py`

```python
"""
çŸ¥è¯†ç®¡ç†ç›¸å…³æç¤ºè¯
"""

from prompts import YA_MCPServer_Prompt


@YA_MCPServer_Prompt(
    name="knowledge_qa",
    title="Knowledge Q&A",
    description="çŸ¥è¯†é—®ç­”åŠ©æ‰‹æç¤ºè¯",
)
async def knowledge_qa_prompt(topic: str) -> str:
    """çŸ¥è¯†é—®ç­”æç¤ºè¯ã€‚

    Args:
        topic (str): æé—®ä¸»é¢˜ã€‚
    Returns:
        str: æç¤ºè¯ã€‚
    """
    return f"è¯·ä»çŸ¥è¯†åº“æŸ¥æ‰¾ã€Œ{topic}ã€ç›¸å…³ä¿¡æ¯å¹¶å›ç­”ã€‚å…ˆç”¨ search_knowledge æœç´¢ï¼Œå†ç”¨ ask_knowledge é—®ç­”ã€‚"


@YA_MCPServer_Prompt(
    name="knowledge_import",
    title="Knowledge Import",
    description="çŸ¥è¯†å¯¼å…¥åŠ©æ‰‹æç¤ºè¯",
)
async def knowledge_import_prompt(topic: str) -> str:
    """çŸ¥è¯†å¯¼å…¥æç¤ºè¯ã€‚

    Args:
        topic (str): çŸ¥è¯†ä¸»é¢˜ã€‚
    Returns:
        str: æç¤ºè¯ã€‚
    """
    return f"æˆ‘è¦å¯¼å…¥å…³äºã€Œ{topic}ã€çš„çŸ¥è¯†ã€‚è¯·å¼•å¯¼æˆ‘æä¾›å†…å®¹ï¼Œç„¶åç”¨ add_knowledge æ·»åŠ ã€‚"
```

---

## B-8: æµ‹è¯•é˜¶æ®µ Bï¼ˆ20åˆ†é’Ÿï¼‰

```powershell
$env:DEEPSEEK_API_KEY="ä½ çš„Key"
uv run server.py
```

MCP Inspector æµ‹è¯•æµç¨‹ï¼š

```
1ï¸âƒ£ add_knowledge
   content: "Python è£…é¥°å™¨æ˜¯ä¸€ç§ä¿®æ”¹å‡½æ•°è¡Œä¸ºçš„è®¾è®¡æ¨¡å¼ã€‚
   æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé«˜é˜¶å‡½æ•°ï¼Œæ¥æ”¶å‡½æ•°ä½œä¸ºå‚æ•°è¿”å›æ–°å‡½æ•°ã€‚
   å¸¸è§å†…ç½®è£…é¥°å™¨ï¼š@propertyã€@staticmethodã€@classmethodã€‚
   è‡ªå®šä¹‰è£…é¥°å™¨ç¤ºä¾‹ï¼š
   def timer(func):
       import time
       def wrapper(*args, **kwargs):
           start = time.time()
           result = func(*args, **kwargs)
           print(f'è€—æ—¶: {time.time()-start:.2f}ç§’')
           return result
       return wrapper"
   title: "Pythonè£…é¥°å™¨æ•™ç¨‹"
   tags: "python,ç¼–ç¨‹,è£…é¥°å™¨"
   source: "è¯¾ä»¶"

2ï¸âƒ£ search_knowledge
   query: "å¦‚ä½•ç»™å‡½æ•°è®¡æ—¶"  â† æ²¡æ"è£…é¥°å™¨"ï¼Œä½†èƒ½æœåˆ°ï¼

3ï¸âƒ£ ask_knowledge
   question: "Pythonä¸­æ€ä¹ˆåœ¨å‡½æ•°å‰åè‡ªåŠ¨æ‰“å°æ—¥å¿—ï¼Ÿ"
   â†’ è‡ªåŠ¨æ£€ç´¢è£…é¥°å™¨çŸ¥è¯† + DeepSeek ç”Ÿæˆå›ç­”

4ï¸âƒ£ knowledge_stats
   â†’ æŸ¥çœ‹ç»Ÿè®¡

5ï¸âƒ£ weather_query (é˜¶æ®µAçš„å·¥å…·ä»ç„¶å¯ç”¨ï¼)
   city: "åŒ—äº¬"
```

---

## B-9: ä»£ç è§„èŒƒ + Git æäº¤ï¼ˆ15åˆ†é’Ÿï¼‰

```powershell
# ä»£ç è§„èŒƒ
uv run ruff check .
uv run ruff check . --fix
uv run black .

# Git æäº¤
git add core/knowledge_store.py core/rag_service.py core/document_processor.py
git commit -m "feat: add ChromaDB knowledge store, RAG service, text chunking"

git add tools/knowledge_tool.py tools/qa_tool.py
git commit -m "feat: add knowledge management and RAG Q&A tools"

git add resources/knowledge_resource.py prompts/knowledge_prompt.py
git commit -m "feat: add knowledge guide resource and prompts"

git add config.yaml pyproject.toml .gitignore
git commit -m "chore: add chromadb dependency, knowledge config"
```

---

## B-10: æ›´æ–° README.md + åˆå¹¶åˆ° mainï¼ˆ15åˆ†é’Ÿï¼‰

æ›´æ–° README.md å†…å®¹ï¼ˆå‚ç…§ `å¼€å‘æŒ‡å—_å®Œæ•´æµç¨‹.md` çš„ Step 11ï¼‰ï¼Œç„¶åï¼š

```powershell
git add README.md
git commit -m "docs: update README with full project documentation"

# åˆå¹¶åˆ° main
git checkout main
git merge dev
git log --oneline
```

---

## âœ… å…¨éƒ¨å®Œæˆï¼

### æœ€ç»ˆé¡¹ç›®åŒ…å«ï¼š

| æ¥è‡ªé˜¶æ®µ | æ–‡ä»¶ | åŠŸèƒ½ |
|---------|------|------|
| A | `core/llm_service.py` | LLM å¯¹è¯ï¼ˆDeepSeek/OpenAIï¼‰|
| A | `core/weather_service.py` | å¤©æ°”æŸ¥è¯¢ |
| A | `core/translate_service.py` | æ–‡æœ¬ç¿»è¯‘ |
| A | `tools/chat_tool.py` | å¯¹è¯å·¥å…· |
| A | `tools/weather_tool.py` | å¤©æ°”å·¥å…· |
| A | `tools/translate_tool.py` | ç¿»è¯‘å·¥å…· |
| **B** | **`core/knowledge_store.py`** | **å‘é‡æ•°æ®åº“æ“ä½œ** |
| **B** | **`core/rag_service.py`** | **RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ** |
| **B** | **`core/document_processor.py`** | **æ–‡æœ¬åˆ†å—** |
| **B** | **`tools/knowledge_tool.py`** | **çŸ¥è¯†ç®¡ç†å·¥å…· Ã—4** |
| **B** | **`tools/qa_tool.py`** | **RAG é—®ç­”å·¥å…· Ã—2** |
| B | `resources/knowledge_resource.py` | ä½¿ç”¨æŒ‡å— |
| B | `prompts/knowledge_prompt.py` | çŸ¥è¯†ç®¡ç†æç¤ºè¯ |

**æ€»è®¡ï¼š8 ä¸ª Tool + 3 ä¸ª Resource + 5 ä¸ª Prompt**

### æ—¶é—´çº¿æ€»ç»“ï¼š

```
Day 1ï¼ˆ2-3å°æ—¶ï¼‰: é˜¶æ®µ A
  â†’ ç¯å¢ƒæ­å»º + LLM/å¤©æ°”/ç¿»è¯‘ ä¸‰ä¸ªå·¥å…·æå®š
  â†’ è·‘é€š MCP Inspector æµ‹è¯•
  â†’ âœ… å·²ç»å¯ä»¥ä½œä¸ºåŸºç¡€é¢˜æäº¤

Day 2ï¼ˆ4-5å°æ—¶ï¼‰: é˜¶æ®µ B
  â†’ åŠ  ChromaDB + RAG + çŸ¥è¯†ç®¡ç†å·¥å…·
  â†’ å®Œå–„ README + Git è§„èŒƒ
  â†’ âœ… å‡çº§ä¸ºä¸­ç­‰éš¾åº¦æäº¤ï¼Œæ‹¿é«˜åˆ†
```

---

## âš¡ å¿«é€Ÿæ£€æŸ¥æ¸…å•

æäº¤å‰ç¡®è®¤ï¼š

- [ ] `config.yaml` é¡¹ç›®åå’Œæè¿°å·²ä¿®æ”¹
- [ ] `pyproject.toml` ä¸ config.yaml ä¸€è‡´
- [ ] `README.md` å¡«å†™å®Œæ•´ï¼ˆç»„å‘˜ä¿¡æ¯ã€Tool/Resource/Prompt åˆ—è¡¨ï¼‰
- [ ] API Key æ²¡æœ‰ç¡¬ç¼–ç åœ¨ä»£ç é‡Œ
- [ ] `data/` åœ¨ .gitignore ä¸­
- [ ] Git æœ‰å¤šæ¬¡æœ‰æ„ä¹‰çš„ commit
- [ ] main åˆ†æ”¯åŒ…å«æœ€ç»ˆä»£ç 
- [ ] `uv run ruff check .` æ— æŠ¥é”™
- [ ] MCP Inspector æµ‹è¯•å…¨éƒ¨é€šè¿‡
