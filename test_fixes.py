"""
æµ‹è¯• URL æ¸…ç†å’Œ list_knowledge åŠŸèƒ½
"""

import asyncio


async def test_url_cleaning():
    """æµ‹è¯• URL çš„ç©ºæ ¼å’Œæ¢è¡Œç¬¦å¤„ç†"""
    print("\n=== æµ‹è¯• URL æ¸…ç†åŠŸèƒ½ ===")
    
    try:
        from tools.document_tool import import_webpage
        
        # æµ‹è¯•å¸¦ç©ºæ ¼å’Œæ¢è¡Œç¬¦çš„ URL
        test_urls = [
            "  https://example.com  ",
            "https://example.com\n",
            "\nhttps://example.com",
            "  https://example.com\n\n  ",
        ]
        
        for url in test_urls:
            print(f"\næµ‹è¯• URL: {repr(url)}")
            try:
                result = await import_webpage(url=url, tags="æµ‹è¯•")
                print(f"âœ… æˆåŠŸå¯¼å…¥: {result['title']}")
                print(f"   ID: {result['id']}")
            except Exception as e:
                print(f"âŒ å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


async def test_list_all_knowledge():
    """æµ‹è¯•åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†ï¼ˆåŒ…æ‹¬ PDF å’Œç½‘é¡µï¼‰"""
    print("\n=== æµ‹è¯• list_knowledge åŠŸèƒ½ ===")
    
    try:
        from core.knowledge_store import list_knowledge
        
        # åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†
        result = await list_knowledge(limit=100)
        
        print(f"\nğŸ“š çŸ¥è¯†åº“ç»Ÿè®¡:")
        print(f"   æ€»æ¡ç›®æ•°: {result['total_items']}")
        print(f"   æ€»åˆ†å—æ•°: {result['total_chunks']}")
        
        print(f"\nğŸ“‹ æ‰€æœ‰çŸ¥è¯†æ¡ç›®:")
        for idx, item in enumerate(result['items'], 1):
            print(f"\n{idx}. {item['title']}")
            print(f"   ID: {item['id']}")
            print(f"   æ¥æº: {item['source']}")
            print(f"   æ ‡ç­¾: {item['tags']}")
            print(f"   åˆ†å—æ•°: {item['total_chunks']}")
            print(f"   é¢„è§ˆ: {item['preview'][:50]}...")
        
        if result['total_items'] == 0:
            print("\nğŸ’¡ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆå¯¼å…¥ä¸€äº›æ–‡æ¡£")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


async def test_search_knowledge():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print("\n=== æµ‹è¯• search_knowledge åŠŸèƒ½ ===")
    
    try:
        from core.knowledge_store import search_knowledge
        
        # æœç´¢å…³é”®è¯
        queries = ["example", "æµ‹è¯•", "ç½‘é¡µ", "PDF"]
        
        for query in queries:
            print(f"\nğŸ” æœç´¢: '{query}'")
            result = await search_knowledge(query=query, top_k=5)
            
            print(f"   æ‰¾åˆ° {result['total_results']} æ¡ç»“æœ")
            
            for idx, item in enumerate(result['results'][:3], 1):
                print(f"   {idx}. {item['title']} (ç›¸ä¼¼åº¦: {item['relevance']})")
                print(f"      æ¥æº: {item['source']}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


async def test_pdf_path_cleaning():
    """æµ‹è¯• PDF è·¯å¾„æ¸…ç†"""
    print("\n=== æµ‹è¯• PDF è·¯å¾„æ¸…ç†åŠŸèƒ½ ===")
    
    try:
        from core.file_parser import PDFParser
        
        parser = PDFParser()
        
        # æµ‹è¯•å¸¦ç©ºæ ¼çš„è·¯å¾„è¯†åˆ«
        test_paths = [
            "  test.pdf  ",
            "test.pdf\n",
            "\n  /path/to/file.pdf  \n",
        ]
        
        for path in test_paths:
            print(f"\næµ‹è¯•è·¯å¾„: {repr(path)}")
            # å…ˆ strip å†æ£€æŸ¥
            cleaned = path.strip()
            can_handle = parser.can_handle(cleaned)
            print(f"   æ¸…ç†å: {repr(cleaned)}")
            print(f"   èƒ½å¤„ç†: {can_handle}")
        
        print("\nâœ… PDF è·¯å¾„æ¸…ç†æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("URL æ¸…ç†å’ŒçŸ¥è¯†åˆ—è¡¨åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯• PDF è·¯å¾„æ¸…ç†
    await test_pdf_path_cleaning()
    
    # æµ‹è¯• URL æ¸…ç†ï¼ˆå®é™…å¯¼å…¥ï¼‰
    # await test_url_cleaning()  # å–æ¶ˆæ³¨é‡Šä»¥æµ‹è¯•å®é™…å¯¼å…¥
    
    # æµ‹è¯•åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†
    await test_list_all_knowledge()
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    await test_search_knowledge()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    
    print("\nğŸ“ ä¿®å¤è¯´æ˜:")
    print("1. âœ… URL å’Œæ–‡ä»¶è·¯å¾„ä¼šè‡ªåŠ¨å»é™¤ç©ºæ ¼å’Œæ¢è¡Œç¬¦")
    print("2. âœ… list_knowledge é»˜è®¤ limit æå‡åˆ° 100")
    print("3. âœ… list_knowledge ä¼˜åŒ–äº†å»é‡é€»è¾‘")
    print("4. âœ… æ‰€æœ‰å¯¼å…¥çš„æ–‡æ¡£éƒ½åº”è¯¥èƒ½è¢«æœç´¢åˆ°")


if __name__ == "__main__":
    asyncio.run(main())
