"""
LLM æœåŠ¡æ¨¡å— - æ”¯æŒå¤š Provider

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- chat_with_llm: è°ƒç”¨ LLM API è¿›è¡Œå¯¹è¯ï¼ˆæ”¯æŒ deepseek / openai / siliconflowï¼‰
- generate_metadata: è°ƒç”¨ AI è‡ªåŠ¨ç”ŸæˆçŸ¥è¯†çš„æ ‡é¢˜ã€æ ‡ç­¾ã€æ¥æºç±»å‹
- summarize_content: è°ƒç”¨ AI å¯¹æ–‡æ¡£å†…å®¹ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦æ€»ç»“
"""

import asyncio
import json
import os
from typing import Dict, Any, Optional, List
from openai import AsyncOpenAI
from modules.YA_Common.utils.config import get_config
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("llm_service")

# æ¯ä¸ª provider çš„ API Key ç¼“å­˜
_api_key_cache: Dict[str, str] = {}

# æ¯ä¸ª provider çš„é»˜è®¤é…ç½®
_PROVIDER_DEFAULTS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com",
        "model": "deepseek-chat",
        "env_var": "DEEPSEEK_API_KEY",
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "model": "gpt-4o-mini",
        "env_var": "OPENAI_API_KEY",
    },
    "siliconflow": {
        "base_url": "https://api.siliconflow.cn/v1",
        "model": "deepseek-ai/DeepSeek-V3",
        "env_var": "SILICONFLOW_API_KEY",
    },
}


def _get_api_key_for_provider(provider: str) -> str:
    """è·å–æŒ‡å®š provider çš„ API Keyï¼Œä¼˜å…ˆ SOPSï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ã€‚ç»“æœæŒ‰ provider ç¼“å­˜ã€‚"""
    if provider in _api_key_cache:
        return _api_key_cache[provider]

    sops_key_name = f"{provider}_api_key"
    env_var = _PROVIDER_DEFAULTS.get(provider, {}).get("env_var", f"{provider.upper()}_API_KEY")

    # ä¼˜å…ˆ SOPS
    try:
        from modules.YA_Secrets.secrets_parser import get_secret
        key = get_secret(sops_key_name)
        if key:
            logger.debug(f"ä» SOPS è·å– {provider} API Key æˆåŠŸ")
            _api_key_cache[provider] = key
            return key
    except Exception as e:
        logger.warning(f"SOPS è·å– {provider} API Key å¤±è´¥: {e}")

    # å¤‡ç”¨ï¼šç¯å¢ƒå˜é‡
    key = os.environ.get(env_var)
    if key:
        logger.debug(f"ä»ç¯å¢ƒå˜é‡ {env_var} è·å– {provider} API Key")
        _api_key_cache[provider] = key
        return key

    raise RuntimeError(
        f"æœªæ‰¾åˆ° {provider} API Keyï¼Œè¯·å°†å…¶åŠ å¯†åˆ° env.yamlï¼ˆkey: {sops_key_name}ï¼‰æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ {env_var}"
    )


def _get_provider_config(provider: str) -> Dict[str, Any]:
    """ä» config.yaml å’Œå†…ç½®é»˜è®¤å€¼è·å– provider é…ç½®ã€‚"""
    defaults = _PROVIDER_DEFAULTS.get(provider, {})
    return {
        "base_url": get_config(f"llm.{provider}.base_url", defaults.get("base_url", "")),
        "model": get_config(f"llm.{provider}.model", defaults.get("model", "")),
        "max_tokens": get_config(f"llm.{provider}.max_tokens", 2048),
        "temperature": get_config(f"llm.{provider}.temperature", 0.7),
    }


async def chat_with_llm(
    message: str,
    system_prompt: Optional[str] = None,
    provider: Optional[str] = None,
) -> Dict[str, Any]:
    """
    è°ƒç”¨ LLM API è¿›è¡Œå¯¹è¯ã€‚æ”¯æŒ deepseek / openai / siliconflowã€‚

    Args:
        message (str): ç”¨æˆ·æ¶ˆæ¯ã€‚
        system_prompt (Optional[str]): ç³»ç»Ÿæç¤ºè¯ã€‚
        provider (Optional[str]): LLM æä¾›å•†ã€‚ç•™ç©ºåˆ™è¯»å– config.yaml çš„ llm.default_providerã€‚

    Returns:
        Dict[str, Any]: {"provider", "model", "reply", "usage"}
    """
    effective_provider = provider or get_config("llm.default_provider", "deepseek")
    logger.info(f"è°ƒç”¨ LLM [{effective_provider}]ï¼Œæ¶ˆæ¯é•¿åº¦: {len(message)}")

    try:
        api_key = await asyncio.to_thread(_get_api_key_for_provider, effective_provider)
        cfg = _get_provider_config(effective_provider)
    except Exception as e:
        raise RuntimeError(f"è¯»å– LLM é…ç½®å¤±è´¥: {e}")

    if not cfg["base_url"] or not cfg["model"]:
        raise RuntimeError(f"provider '{effective_provider}' çš„ base_url æˆ– model æœªé…ç½®")

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=cfg["base_url"])

        messages: List[Dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})

        response = await client.chat.completions.create(
            model=cfg["model"],
            messages=messages,
            max_tokens=cfg["max_tokens"],
            temperature=cfg["temperature"],
        )

        reply = response.choices[0].message.content
        usage = {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
        }
        logger.info(f"LLM [{effective_provider}] å›å¤æˆåŠŸï¼ŒToken: {usage}")
        return {"provider": effective_provider, "model": cfg["model"], "reply": reply, "usage": usage}
    except Exception as e:
        raise RuntimeError(f"LLM [{effective_provider}] è°ƒç”¨å¤±è´¥: {e}")


# å±•é™¢å…¼å®¹æ—§ä»£ç çš„å†…éƒ¨å¸®æ°”å‡½æ•°
def _get_api_key() -> str:
    """å¯†å°å…¼å®¹ï¼šä¸º generate_metadata / summarize_content æä¾› deepseek keyã€‚"""
    return _get_api_key_for_provider("deepseek")


METADATA_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ç®¡ç†åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆä»¥ä¸‹å…ƒæ•°æ®ï¼š

1. title: ç®€æ´çš„ä¸­æ–‡æ ‡é¢˜ï¼ˆ10å­—ä»¥å†…ï¼‰ï¼Œæ¦‚æ‹¬æ–‡æœ¬ä¸»é¢˜
2. tags: 3-5ä¸ªæ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”ï¼Œåæ˜ æ–‡æœ¬çš„å…³é”®ä¸»é¢˜å’Œé¢†åŸŸï¼ˆå¦‚ "Python,è£…é¥°å™¨,ç¼–ç¨‹"ï¼‰
3. source: æ–‡æœ¬æ¥æºç±»å‹ï¼Œä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªï¼šè¯¾ä»¶ã€ç¬”è®°ã€è®ºæ–‡ã€æ•™æã€æ–‡æ¡£ã€åšå®¢ã€ä»£ç ã€å…¶ä»–

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼š
{"title": "...", "tags": "...", "source": "..."}"""


async def generate_metadata(content: str) -> Dict[str, str]:
    """
    è°ƒç”¨ DeepSeek ä¸ºçŸ¥è¯†å†…å®¹è‡ªåŠ¨ç”Ÿæˆ titleã€tagsã€sourceã€‚

    Args:
        content (str): çŸ¥è¯†åŸæ–‡ï¼ˆå–å‰ 1500 å­—é€ç»™ AIï¼‰ã€‚

    Returns:
        Dict[str, str]: {"title": "...", "tags": "...", "source": "..."}
    """
    # æˆªå–å‰ 1500 å­—ï¼Œé¿å… token è¿‡é•¿
    preview = content[:1500]
    logger.info(f"AI ç”Ÿæˆå…ƒæ•°æ®ï¼Œå†…å®¹é¢„è§ˆé•¿åº¦: {len(preview)}")

    try:
        api_key = await asyncio.to_thread(_get_api_key)
        base_url = get_config("llm.deepseek.base_url", "https://api.deepseek.com")
        model = get_config("llm.deepseek.model", "deepseek-chat")
    except Exception as e:
        raise RuntimeError(f"è¯»å– LLM é…ç½®å¤±è´¥: {e}")

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": METADATA_SYSTEM_PROMPT},
                {"role": "user", "content": f"è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆå…ƒæ•°æ®ï¼š\n\n{preview}"},
            ],
            max_tokens=200,
            temperature=0.3,
        )

        reply = response.choices[0].message.content.strip()
        # å°è¯•ä»å›å¤ä¸­æå– JSON
        # æœ‰æ—¶ AI ä¼šè¿”å› ```json ... ```ï¼Œéœ€è¦æ¸…ç†
        if "```" in reply:
            reply = reply.split("```")[1]
            if reply.startswith("json"):
                reply = reply[4:]
            reply = reply.strip()

        result = json.loads(reply)
        logger.info(f"AI å…ƒæ•°æ®ç”ŸæˆæˆåŠŸ: {result}")
        return {
            "title": result.get("title", "æœªå‘½å"),
            "tags": result.get("tags", ""),
            "source": result.get("source", "ç”¨æˆ·ç¬”è®°"),
        }
    except json.JSONDecodeError:
        logger.warning(f"AI è¿”å›çš„ JSON è§£æå¤±è´¥: {reply}")
        return {"title": "æœªå‘½å", "tags": "", "source": "ç”¨æˆ·ç¬”è®°"}
    except Exception as e:
        raise RuntimeError(f"AI ç”Ÿæˆå…ƒæ•°æ®å¤±è´¥: {e}")


SUMMARY_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ€»ç»“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ Markdown æ‘˜è¦ã€‚

## è¦æ±‚ï¼š
1. æ‘˜è¦åº”åŒ…å«ï¼šæ–‡æ¡£ä¸»é¢˜ã€æ ¸å¿ƒè¦ç‚¹ï¼ˆ3-7 æ¡ï¼‰ã€å…³é”®ç»“è®º
2. ä½¿ç”¨ Markdown æ ¼å¼ï¼Œå±‚æ¬¡æ¸…æ™°
3. è¯­è¨€ç®€æ´ç²¾ç‚¼ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
4. å¦‚æœå†…å®¹åŒ…å«æ•°æ®æˆ–æ¡ˆä¾‹ï¼Œé€‚å½“å¼•ç”¨
5. æ‘˜è¦é•¿åº¦æ§åˆ¶åœ¨ 100-500 å­—

## è¾“å‡ºæ ¼å¼ï¼š
# ğŸ“„ æ–‡æ¡£æ‘˜è¦ï¼š{æ–‡æ¡£æ ‡é¢˜}

## ä¸»é¢˜æ¦‚è¿°
ï¼ˆä¸€å¥è¯æ¦‚æ‹¬æ–‡æ¡£ä¸»é¢˜ï¼‰

## æ ¸å¿ƒè¦ç‚¹
- è¦ç‚¹ 1
- è¦ç‚¹ 2
- ...

## å…³é”®ç»“è®º
ï¼ˆæ€»ç»“æ€§ç»“è®ºï¼‰
"""


async def summarize_content(content: str, title: str = "") -> str:
    """
    è°ƒç”¨ DeepSeek å¯¹æ–‡æ¡£å†…å®¹ç”Ÿæˆç»“æ„åŒ– Markdown æ‘˜è¦ã€‚

    Args:
        content (str): æ–‡æ¡£åŸæ–‡å†…å®¹ã€‚
        title (str): æ–‡æ¡£æ ‡é¢˜ï¼ˆç”¨äºæç¤º AIï¼‰ã€‚

    Returns:
        str: Markdown æ ¼å¼çš„æ‘˜è¦æ–‡æœ¬ã€‚
    """
    # æˆªå–å‰ 6000 å­—ï¼Œå¹³è¡¡æ‘˜è¦è´¨é‡å’Œ token æ¶ˆè€—
    preview = content[:6000]
    logger.info(f"AI ç”Ÿæˆæ‘˜è¦ï¼Œå†…å®¹é¢„è§ˆé•¿åº¦: {len(preview)}, æ ‡é¢˜: {title}")

    try:
        api_key = await asyncio.to_thread(_get_api_key)
        base_url = get_config("llm.deepseek.base_url", "https://api.deepseek.com")
        model = get_config("llm.deepseek.model", "deepseek-chat")
    except Exception as e:
        logger.warning(f"è¯»å– LLM é…ç½®å¤±è´¥ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ: {e}")
        return ""

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)

        user_msg = f"è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ï¼š\n\n"
        if title:
            user_msg += f"æ–‡æ¡£æ ‡é¢˜ï¼š{title}\n\n"
        user_msg += f"æ–‡æ¡£å†…å®¹ï¼š\n{preview}"

        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SUMMARY_SYSTEM_PROMPT},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=1024,
            temperature=0.5,
        )

        summary = response.choices[0].message.content.strip()
        usage = response.usage
        logger.info(
            f"AI æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)}ï¼Œ"
            f"Token: prompt={usage.prompt_tokens}, completion={usage.completion_tokens}"
        )
        return summary

    except Exception as e:
        logger.warning(f"AI æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡: {e}")
        return ""