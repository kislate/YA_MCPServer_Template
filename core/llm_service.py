"""
LLM æœåŠ¡æ¨¡å— - DeepSeek

æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- chat_with_llm: è°ƒç”¨ DeepSeek API è¿›è¡Œå¯¹è¯
- generate_metadata: è°ƒç”¨ AI è‡ªåŠ¨ç”ŸæˆçŸ¥è¯†çš„æ ‡é¢˜ã€æ ‡ç­¾ã€æ¥æºç±»å‹
- summarize_content: è°ƒç”¨ AI å¯¹æ–‡æ¡£å†…å®¹ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦æ€»ç»“
"""

import asyncio
import json
import os
from typing import Dict, Any, Optional, List
from openai import AsyncOpenAI
from modules.YA_Common.utils.config import get_config
from modules.YA_Common.utils.logger import get_logger

logger = get_logger("llm_service")

# ç¼“å­˜ API keyï¼Œé¿å…æ¯æ¬¡è°ƒç”¨éƒ½è§£å¯† SOPS
_cached_api_key: Optional[str] = None


async def chat_with_llm(
    message: str,
    system_prompt: Optional[str] = None,
    provider: Optional[str] = None,
) -> Dict[str, Any]:
    """
    è°ƒç”¨ DeepSeek API è¿›è¡Œå¯¹è¯ã€‚

    Args:
        message (str): ç”¨æˆ·æ¶ˆæ¯ã€‚
        system_prompt (Optional[str]): ç³»ç»Ÿæç¤ºè¯ã€‚
        provider (Optional[str]): ä¿ç•™å‚æ•°ï¼Œå½“å‰ä»…æ”¯æŒ "deepseek"ã€‚

    Returns:
        Dict[str, Any]: å¯¹è¯ç»“æœï¼ŒåŒ…å«å›å¤å†…å®¹å’Œ Token ä½¿ç”¨ä¿¡æ¯ã€‚

    Raises:
        RuntimeError: å¦‚æœ API è°ƒç”¨å¤±è´¥ã€‚

    Example:
        {
            "provider": "deepseek",
            "model": "deepseek-chat",
            "reply": "Python çš„è£…é¥°å™¨æ˜¯...",
            "usage": {"prompt_tokens": 150, "completion_tokens": 200}
        }
    """
    provider = "deepseek"

    logger.info(f"è°ƒç”¨ DeepSeekï¼Œæ¶ˆæ¯é•¿åº¦: {len(message)}")

    try:
        api_key = await asyncio.to_thread(_get_api_key)
        base_url = get_config("llm.deepseek.base_url", "https://api.deepseek.com")
        model = get_config("llm.deepseek.model", "deepseek-chat")
        max_tokens = get_config("llm.deepseek.max_tokens", 2048)
        temperature = get_config("llm.deepseek.temperature", 0.7)
    except Exception as e:
        raise RuntimeError(f"è¯»å– LLM é…ç½®å¤±è´¥: {e}")

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)

        messages: List[Dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})

        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        reply = response.choices[0].message.content
        usage = {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
        }

        logger.info(f"DeepSeek å›å¤æˆåŠŸï¼ŒToken: {usage}")

        return {
            "provider": provider,
            "model": model,
            "reply": reply,
            "usage": usage,
        }
    except Exception as e:
        raise RuntimeError(f"DeepSeek è°ƒç”¨å¤±è´¥: {e}")


def _get_api_key() -> str:
    """ä» SOPS è·å– DeepSeek API Keyï¼Œç¯å¢ƒå˜é‡ä½œä¸ºå¤‡ç”¨ã€‚ç»“æœç¼“å­˜ã€‚"""
    global _cached_api_key
    if _cached_api_key:
        return _cached_api_key

    # ä¼˜å…ˆä» SOPS è·å–
    try:
        from modules.YA_Secrets.secrets_parser import get_secret
        key = get_secret("deepseek_api_key")
        if key:
            logger.debug("ä» SOPS è·å– DeepSeek API Key æˆåŠŸ")
            _cached_api_key = key
            return key
    except Exception as e:
        logger.warning(f"SOPS è·å– API Key å¤±è´¥: {e}")

    # å¤‡ç”¨ï¼šç¯å¢ƒå˜é‡
    key = os.environ.get("DEEPSEEK_API_KEY")
    if key:
        logger.debug("ä»ç¯å¢ƒå˜é‡è·å– DeepSeek API Key")
        return key

    raise RuntimeError(
        "æœªæ‰¾åˆ° DeepSeek API Keyï¼Œè¯·é€šè¿‡ SOPS åŠ å¯†åˆ° env.yaml æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY"
    )


METADATA_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ç®¡ç†åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆä»¥ä¸‹å…ƒæ•°æ®ï¼š

1. title: ç®€æ´çš„ä¸­æ–‡æ ‡é¢˜ï¼ˆ10å­—ä»¥å†…ï¼‰ï¼Œæ¦‚æ‹¬æ–‡æœ¬ä¸»é¢˜
2. tags: 3-5ä¸ªæ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”ï¼Œåæ˜ æ–‡æœ¬çš„å…³é”®ä¸»é¢˜å’Œé¢†åŸŸï¼ˆå¦‚ "Python,è£…é¥°å™¨,ç¼–ç¨‹"ï¼‰
3. source: æ–‡æœ¬æ¥æºç±»å‹ï¼Œä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªï¼šè¯¾ä»¶ã€ç¬”è®°ã€è®ºæ–‡ã€æ•™æã€æ–‡æ¡£ã€åšå®¢ã€ä»£ç ã€å…¶ä»–

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼š
{"title": "...", "tags": "...", "source": "..."}"""


async def generate_metadata(content: str) -> Dict[str, str]:
    """
    è°ƒç”¨ DeepSeek ä¸ºçŸ¥è¯†å†…å®¹è‡ªåŠ¨ç”Ÿæˆ titleã€tagsã€sourceã€‚

    Args:
        content (str): çŸ¥è¯†åŸæ–‡ï¼ˆå–å‰ 1500 å­—é€ç»™ AIï¼‰ã€‚

    Returns:
        Dict[str, str]: {"title": "...", "tags": "...", "source": "..."}
    """
    # æˆªå–å‰ 1500 å­—ï¼Œé¿å… token è¿‡é•¿
    preview = content[:1500]
    logger.info(f"AI ç”Ÿæˆå…ƒæ•°æ®ï¼Œå†…å®¹é¢„è§ˆé•¿åº¦: {len(preview)}")

    try:
        api_key = await asyncio.to_thread(_get_api_key)
        base_url = get_config("llm.deepseek.base_url", "https://api.deepseek.com")
        model = get_config("llm.deepseek.model", "deepseek-chat")
    except Exception as e:
        raise RuntimeError(f"è¯»å– LLM é…ç½®å¤±è´¥: {e}")

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": METADATA_SYSTEM_PROMPT},
                {"role": "user", "content": f"è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆå…ƒæ•°æ®ï¼š\n\n{preview}"},
            ],
            max_tokens=200,
            temperature=0.3,
        )

        reply = response.choices[0].message.content.strip()
        # å°è¯•ä»å›å¤ä¸­æå– JSON
        # æœ‰æ—¶ AI ä¼šè¿”å› ```json ... ```ï¼Œéœ€è¦æ¸…ç†
        if "```" in reply:
            reply = reply.split("```")[1]
            if reply.startswith("json"):
                reply = reply[4:]
            reply = reply.strip()

        result = json.loads(reply)
        logger.info(f"AI å…ƒæ•°æ®ç”ŸæˆæˆåŠŸ: {result}")
        return {
            "title": result.get("title", "æœªå‘½å"),
            "tags": result.get("tags", ""),
            "source": result.get("source", "ç”¨æˆ·ç¬”è®°"),
        }
    except json.JSONDecodeError:
        logger.warning(f"AI è¿”å›çš„ JSON è§£æå¤±è´¥: {reply}")
        return {"title": "æœªå‘½å", "tags": "", "source": "ç”¨æˆ·ç¬”è®°"}
    except Exception as e:
        raise RuntimeError(f"AI ç”Ÿæˆå…ƒæ•°æ®å¤±è´¥: {e}")


SUMMARY_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ€»ç»“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ Markdown æ‘˜è¦ã€‚

## è¦æ±‚ï¼š
1. æ‘˜è¦åº”åŒ…å«ï¼šæ–‡æ¡£ä¸»é¢˜ã€æ ¸å¿ƒè¦ç‚¹ï¼ˆ3-7 æ¡ï¼‰ã€å…³é”®ç»“è®º
2. ä½¿ç”¨ Markdown æ ¼å¼ï¼Œå±‚æ¬¡æ¸…æ™°
3. è¯­è¨€ç®€æ´ç²¾ç‚¼ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
4. å¦‚æœå†…å®¹åŒ…å«æ•°æ®æˆ–æ¡ˆä¾‹ï¼Œé€‚å½“å¼•ç”¨
5. æ‘˜è¦é•¿åº¦æ§åˆ¶åœ¨ 300-800 å­—

## è¾“å‡ºæ ¼å¼ï¼š
# ğŸ“„ æ–‡æ¡£æ‘˜è¦ï¼š{æ–‡æ¡£æ ‡é¢˜}

## ä¸»é¢˜æ¦‚è¿°
ï¼ˆä¸€å¥è¯æ¦‚æ‹¬æ–‡æ¡£ä¸»é¢˜ï¼‰

## æ ¸å¿ƒè¦ç‚¹
- è¦ç‚¹ 1
- è¦ç‚¹ 2
- ...

## å…³é”®ç»“è®º
ï¼ˆæ€»ç»“æ€§ç»“è®ºï¼‰
"""


async def summarize_content(content: str, title: str = "") -> str:
    """
    è°ƒç”¨ DeepSeek å¯¹æ–‡æ¡£å†…å®¹ç”Ÿæˆç»“æ„åŒ– Markdown æ‘˜è¦ã€‚

    Args:
        content (str): æ–‡æ¡£åŸæ–‡å†…å®¹ã€‚
        title (str): æ–‡æ¡£æ ‡é¢˜ï¼ˆç”¨äºæç¤º AIï¼‰ã€‚

    Returns:
        str: Markdown æ ¼å¼çš„æ‘˜è¦æ–‡æœ¬ã€‚
    """
    # æˆªå–å‰ 6000 å­—ï¼Œå¹³è¡¡æ‘˜è¦è´¨é‡å’Œ token æ¶ˆè€—
    preview = content[:6000]
    logger.info(f"AI ç”Ÿæˆæ‘˜è¦ï¼Œå†…å®¹é¢„è§ˆé•¿åº¦: {len(preview)}, æ ‡é¢˜: {title}")

    try:
        api_key = await asyncio.to_thread(_get_api_key)
        base_url = get_config("llm.deepseek.base_url", "https://api.deepseek.com")
        model = get_config("llm.deepseek.model", "deepseek-chat")
    except Exception as e:
        logger.warning(f"è¯»å– LLM é…ç½®å¤±è´¥ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ: {e}")
        return ""

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)

        user_msg = f"è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ï¼š\n\n"
        if title:
            user_msg += f"æ–‡æ¡£æ ‡é¢˜ï¼š{title}\n\n"
        user_msg += f"æ–‡æ¡£å†…å®¹ï¼š\n{preview}"

        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SUMMARY_SYSTEM_PROMPT},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=1024,
            temperature=0.5,
        )

        summary = response.choices[0].message.content.strip()
        usage = response.usage
        logger.info(
            f"AI æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)}ï¼Œ"
            f"Token: prompt={usage.prompt_tokens}, completion={usage.completion_tokens}"
        )
        return summary

    except Exception as e:
        logger.warning(f"AI æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡: {e}")
        return ""